{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # on empeche les warnings d'apparaitre\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from scipy.stats import shapiro\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Import du fichier CSV\n",
    "df = pd.read_csv('Allocine_v2_9.csv')\n",
    "\n",
    "# Liste des colonnes à explorer\n",
    "colonnes = ['Famille', 'Comédie musicale', 'Drama', 'Musical', 'Comédie dramatique', 'Opéra', 'Western', 'Action', 'Concert', 'Aventure', 'Historique', 'Biopic', 'Guerre', 'Erotique',\n",
    "            'Drame', 'Documentaire', 'Bollywood', 'Divers', 'Fantastique', 'Spectacle', 'Péplum', 'Espionnage', 'Animation', 'Romance', 'Comédie', 'Policier', 'Arts Martiaux', 'Epouvante-horreur',\n",
    "            'Expérimental', 'Évènement Sportif', 'Thriller', 'Science Fiction', 'Judiciaire']\n",
    "\n",
    "# Parcourir chaque colonne et afficher le nombre d'occurrences de chaque variable\n",
    "for colonne in colonnes:\n",
    "    occurrences = df[colonne].value_counts()\n",
    "    print(\"Nombre d'occurrences dans la colonne '{}':\\n{}\".format(colonne, occurrences))\n",
    "\n",
    "# Supprimer les colonnes avec moins de 30 occurrences du dataframe df\n",
    "df = df.drop(columns=['Opéra', 'Spectacle', 'Drama', 'Concert', 'Divers', 'Péplum', 'Expérimental', 'Évènement Sportif', 'Erotique', 'Bollywood', 'Arts Martiaux', 'Western', 'Judiciaire'])\n",
    "\n",
    "# Calculer la matrice de corrélation\n",
    "corr = df.corr()\n",
    "\n",
    "# Créer le heatmap avec seaborn\n",
    "plt.figure(figsize=(26, 12))\n",
    "heatmap = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Rotation des étiquettes des abscisses en diagonal\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "\n",
    "plt.title('Heatmap de corrélation')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "####################################\n",
    "# Suppression des valeurs extremes #\n",
    "####################################\n",
    "\n",
    "Q1_budget = df['budget_euro'].quantile(0.25)\n",
    "Q3_budget = df['budget_euro'].quantile(0.75)\n",
    "IQR_budget = Q3_budget - Q1_budget\n",
    "\n",
    "Q1_premiere = df['premiere_semaine_france'].quantile(0.25)\n",
    "Q3_premiere = df['premiere_semaine_france'].quantile(0.75)\n",
    "IQR_premiere = Q3_premiere - Q1_premiere\n",
    "\n",
    "min_budget = Q1_budget - 1.5 * IQR_budget\n",
    "max_budget = Q3_budget + 1.5 * IQR_budget\n",
    "\n",
    "min_premiere = Q1_premiere - 1.5 * IQR_premiere\n",
    "max_premiere = Q3_premiere + 1.5 * IQR_premiere\n",
    "\n",
    "df = df[(df['budget_euro'] >= min_budget) & (df['budget_euro'] <= max_budget) & (df['premiere_semaine_france'] >= min_premiere) & (df['premiere_semaine_france'] <= max_premiere)]\n",
    "\n",
    "####################################\n",
    "\n",
    "# Séparation des variables explicatives de la variable cible\n",
    "X = df.drop(columns=['premiere_semaine_france'])\n",
    "y = df['premiere_semaine_france']\n",
    "\n",
    "# Séparation des jeux d'entrainement et de test (test_size=0.2 et random_state=42 vu avec Manon et Sylvain)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "display(X_train)\n",
    "\n",
    "# Convertir les colonnes booléennes en entiers (0 pour False, 1 pour True)\n",
    "boolean_columns = ['USA', 'France', 'Famille','Comédie musicale', 'Musical', 'Comédie dramatique', 'Action', 'Aventure', 'Historique', 'Biopic', 'Guerre', 'Drame', 'Documentaire', 'Fantastique', 'Espionnage', 'Animation', 'Romance', 'Comédie', 'Policier',\n",
    "                   'Epouvante-horreur', 'Thriller', 'Science Fiction']\n",
    "\n",
    "for col in boolean_columns:\n",
    "    X_train[col] = X_train[col].astype(int)\n",
    "\n",
    "# Standardisation des données pour le VIF\n",
    "scaler_vif = StandardScaler()\n",
    "X_train_scaled = scaler_vif.fit_transform(X_train)\n",
    "\n",
    "# Calculer le VIF pour chaque caractéristique dans X_train_scaled\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_scaled, i) for i in range(len(X_train.columns))]\n",
    "\n",
    "# Standardisation des données pour la modélisation\n",
    "scaler = StandardScaler()\n",
    "X_train[['budget_euro', 'acteur', 'realisateur', 'scenariste', 'distributeur', 'note_presse', 'duree']] = scaler.fit_transform(X_train[['budget_euro', 'acteur', 'realisateur', 'scenariste', 'distributeur', 'note_presse', 'duree']])\n",
    "X_test[['budget_euro', 'acteur', 'realisateur', 'scenariste', 'distributeur', 'note_presse', 'duree']] = scaler.transform(X_test[['budget_euro', 'acteur', 'realisateur', 'scenariste', 'distributeur', 'note_presse', 'duree']])\n",
    "\n",
    "# Afficher les résultats\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un modèle DecisionTreeRegressor\n",
    "models = DecisionTreeRegressor(max_depth=5, min_samples_leaf=4, random_state=42)\n",
    "\n",
    "# Entraînement du modèle sur les données d'apprentissage\n",
    "models.fit(X_train, y_train)\n",
    "# Prédiction des valeurs cibles pour l'ensemble de test (X_test)\n",
    "y_pred = models.predict(X_test)\n",
    "\n",
    "# Affichage MAE, R2 et analyse des residus avec Shapiro Wilk\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2_scores = r2_score(y_test, y_pred)\n",
    "residus = y_test - y_pred\n",
    "statistic, p_value = shapiro(residus)\n",
    "\n",
    "print(f'Statistique W={statistic}, p-value={p_value}')\n",
    "if p_value > 0.05:\n",
    "    print(\"L'échantillon des résidus semble provenir d'une distribution normale\")\n",
    "else:\n",
    "    print(\"L'échantillon des résidus ne semble pas provenir d'une distribution normale\")\n",
    "print('-------------------------------')\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {r2_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un modèle RandomForestRegressor\n",
    "models = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Entraînement du modèle sur les données d'apprentissage\n",
    "models.fit(X_train, y_train)\n",
    "# Prédiction des valeurs cibles pour l'ensemble de test (X_test)\n",
    "y_pred = models.predict(X_test)\n",
    "\n",
    "# Affichage MAE, R2 et analyse des residus avec Shapiro Wilk\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2_scores = r2_score(y_test, y_pred)\n",
    "residus = y_test - y_pred\n",
    "statistic, p_value = shapiro(residus)\n",
    "\n",
    "print(f'Statistique W={statistic}, p-value={p_value}')\n",
    "if p_value > 0.05:\n",
    "    print(\"L'échantillon des résidus semble provenir d'une distribution normale\")\n",
    "else:\n",
    "    print(\"L'échantillon des résidus ne semble pas provenir d'une distribution normale\")\n",
    "print('-------------------------------')\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {r2_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un modèle LinearRegression\n",
    "models = LinearRegression()\n",
    "\n",
    "# Entraînement du modèle sur les données d'apprentissage\n",
    "models.fit(X_train, y_train)\n",
    "# Prédiction des valeurs cibles pour l'ensemble de test (X_test)\n",
    "y_pred = models.predict(X_test)\n",
    "\n",
    "# Affichage MAE, R2 et analyse des residus avec Shapiro Wilk\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2_scores = r2_score(y_test, y_pred)\n",
    "residus = y_test - y_pred\n",
    "statistic, p_value = shapiro(residus)\n",
    "\n",
    "print(f'Statistique W={statistic}, p-value={p_value}')\n",
    "if p_value > 0.05:\n",
    "    print(\"L'échantillon des résidus semble provenir d'une distribution normale\")\n",
    "else:\n",
    "    print(\"L'échantillon des résidus ne semble pas provenir d'une distribution normale\")\n",
    "print('-------------------------------')\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {r2_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un modèle GradientBoostingRegressor\n",
    "models = GradientBoostingRegressor(max_depth=5, random_state=42)\n",
    "\n",
    "# Entraînement du modèle sur les données d'apprentissage\n",
    "models.fit(X_train, y_train)\n",
    "# Prédiction des valeurs cibles pour l'ensemble de test (X_test)\n",
    "y_pred = models.predict(X_test)\n",
    "\n",
    "# Affichage MAE, R2 et analyse des residus avec Shapiro Wilk\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2_scores = r2_score(y_test, y_pred)\n",
    "residus = y_test - y_pred\n",
    "statistic, p_value = shapiro(residus)\n",
    "\n",
    "print(f'Statistique W={statistic}, p-value={p_value}')\n",
    "if p_value > 0.05:\n",
    "    print(\"L'échantillon des résidus semble provenir d'une distribution normale\")\n",
    "else:\n",
    "    print(\"L'échantillon des résidus ne semble pas provenir d'une distribution normale\")\n",
    "print('-------------------------------')\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {r2_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un modèle XGBRegressor\n",
    "models = XGBRegressor(objective ='reg:squarederror', max_depth = 5, random_state=42)\n",
    "\n",
    "# Entraînement du modèle sur les données d'apprentissage\n",
    "models.fit(X_train, y_train)\n",
    "# Prédiction des valeurs cibles pour l'ensemble de test (X_test)\n",
    "y_pred = models.predict(X_test)\n",
    "\n",
    "# Affichage MAE, R2 et analyse des residus avec Shapiro Wilk\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2_scores = r2_score(y_test, y_pred)\n",
    "residus = y_test - y_pred\n",
    "statistic, p_value = shapiro(residus)\n",
    "\n",
    "print(f'Statistique W={statistic}, p-value={p_value}')\n",
    "if p_value > 0.05:\n",
    "    print(\"L'échantillon des résidus semble provenir d'une distribution normale\")\n",
    "else:\n",
    "    print(\"L'échantillon des résidus ne semble pas provenir d'une distribution normale\")\n",
    "print('-------------------------------')\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {r2_scores}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
