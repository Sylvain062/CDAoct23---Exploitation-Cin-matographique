{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # on empeche les warnings d'apparaitre\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Import du fichier CSV\n",
    "df = pd.read_csv('Allocine_v2_9.csv')\n",
    "\n",
    "# Liste des colonnes à explorer\n",
    "colonnes = ['Famille', 'Comédie musicale', 'Drama', 'Musical', 'Comédie dramatique', 'Opéra', 'Western', 'Action', 'Concert', 'Aventure', 'Historique', 'Biopic', 'Guerre', 'Erotique',\n",
    "            'Drame', 'Documentaire', 'Bollywood', 'Divers', 'Fantastique', 'Spectacle', 'Péplum', 'Espionnage', 'Animation', 'Romance', 'Comédie', 'Policier', 'Arts Martiaux', 'Epouvante-horreur',\n",
    "            'Expérimental', 'Évènement Sportif', 'Thriller', 'Science Fiction', 'Judiciaire']\n",
    "\n",
    "# Supprimer les colonnes avec moins de 30 occurrences du dataframe df\n",
    "df = df.drop(columns=['note_presse', 'Opéra', 'Spectacle', 'Drama', 'Concert', 'Divers', 'Péplum', 'Expérimental', 'Évènement Sportif', 'Erotique', 'Bollywood', 'Arts Martiaux', 'Western', 'Judiciaire'])\n",
    "\n",
    "####################################\n",
    "# Suppression des valeurs extremes #\n",
    "####################################\n",
    "\n",
    "Q1_budget = df['budget_euro'].quantile(0.25)\n",
    "Q3_budget = df['budget_euro'].quantile(0.75)\n",
    "IQR_budget = Q3_budget - Q1_budget\n",
    "\n",
    "Q1_premiere = df['premiere_semaine_france'].quantile(0.25)\n",
    "Q3_premiere = df['premiere_semaine_france'].quantile(0.75)\n",
    "IQR_premiere = Q3_premiere - Q1_premiere\n",
    "\n",
    "min_budget = Q1_budget - 1.5 * IQR_budget\n",
    "max_budget = Q3_budget + 1.5 * IQR_budget\n",
    "\n",
    "min_premiere = Q1_premiere - 1.5 * IQR_premiere\n",
    "max_premiere = Q3_premiere + 1.5 * IQR_premiere\n",
    "\n",
    "df = df[(df['budget_euro'] >= min_budget) & (df['budget_euro'] <= max_budget) & (df['premiere_semaine_france'] >= min_premiere) & (df['premiere_semaine_france'] <= max_premiere)]\n",
    "\n",
    "####################################\n",
    "\n",
    "\n",
    "\n",
    "# Séparation des variables explicatives de la variable cible\n",
    "X = df.drop(columns=['premiere_semaine_france'])\n",
    "y = df['premiere_semaine_france']\n",
    "\n",
    "# Séparation des jeux d'entrainement et de test (test_size=0.2 et random_state=42 vu avec Manon et Sylvain)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "# FACULTATIF // Utilisation de RandomizedSearchCV pour trouver les meilleurs paramètres  #\n",
    "##########################################################################################\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": sp_randint(100, 500),\n",
    "    \"learning_rate\": uniform(0.01, 0.1),\n",
    "    \"max_depth\": sp_randint(3, 10),\n",
    "    \"min_samples_split\": sp_randint(2, 10),\n",
    "    \"min_samples_leaf\": sp_randint(1, 10),\n",
    "    \"subsample\": uniform(0.7, 0.3)\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(gb_model, param_distributions=param_dist,\n",
    "                                   n_iter=500, cv=5, scoring='neg_mean_absolute_error', random_state=42, n_jobs=-1)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_random_params = random_search.best_params_\n",
    "best_random_score = random_search.best_score_\n",
    "\n",
    "print(\"Meilleurs paramètres:\", best_random_params)\n",
    "print(\"Meilleurs MAE:\", abs(best_random_score))\n",
    "\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "# FACULTATIF // Utilisation de GridSearchCV pour trouver les meilleurs paramètres #\n",
    "###################################################################################\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 150],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.15],\n",
    "    \"max_depth\": [4, 5, 6],\n",
    "    \"min_samples_split\": [1, 2, 3],\n",
    "    \"min_samples_leaf\": [1, 2, 3],\n",
    "    \"subsample\": [0.85, 1.0, 1.15]\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(gb_model, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_random_params = grid_search.best_params_\n",
    "best_random_score = grid_search.best_score_\n",
    "\n",
    "print(\"Meilleurs paramètres:\", best_random_params)\n",
    "print(\"Meilleurs MAE:\", abs(best_random_score))\n",
    "\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "# FACULTATIF // Utilisation de GridSearchCV pour trouver les meilleurs paramètres #\n",
    "###################################################################################\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 150],\n",
    "    \"learning_rate\": [0.1, 0.15],\n",
    "    \"max_depth\": [4],\n",
    "    \"min_child_weight\": [2, 3],\n",
    "    \"subsample\": [1.0, 1.15],\n",
    "    \"colsample_bytree\": [0.8, 0.9]\n",
    "}\n",
    "\n",
    "xgb_model = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "grid_search = GridSearchCV(xgb_model, param_grid=param_grid, cv=10, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Meilleurs paramètres:\", best_params)\n",
    "print(\"Meilleur MAE:\", abs(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un modèle GradientBoostingRegressor\n",
    "models = GradientBoostingRegressor(learning_rate=0.1, n_estimators=65, max_depth=5, min_samples_split=2, min_samples_leaf=1, subsample=1, random_state=42)\n",
    "\n",
    "# Entraînement du modèle sur les données d'apprentissage\n",
    "models.fit(X_train, y_train)\n",
    "# Prédiction des valeurs cibles pour l'ensemble de test (X_test)\n",
    "y_pred = models.predict(X_test)\n",
    "\n",
    "# Affichage MAE, R2 et analyse des residus avec Shapiro Wilk\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2_scores = r2_score(y_test, y_pred)\n",
    "residus = y_test - y_pred\n",
    "statistic, p_value = shapiro(residus)\n",
    "\n",
    "print(f'Statistique W={statistic}, p-value={p_value}')\n",
    "if p_value > 0.05:\n",
    "    print(\"L'échantillon des résidus semble provenir d'une distribution normale\")\n",
    "else:\n",
    "    print(\"L'échantillon des résidus ne semble pas provenir d'une distribution normale\")\n",
    "print('-------------------------------')\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {r2_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un modèle XGBRegressor\n",
    "models = XGBRegressor(objective='reg:squarederror', max_depth=4, learning_rate=0.30, n_estimators=100, subsample=1, min_child_weight=1, random_state=42)\n",
    "\n",
    "# Entraînement du modèle sur les données d'apprentissage\n",
    "models.fit(X_train, y_train)\n",
    "# Prédiction des valeurs cibles pour l'ensemble de test (X_test)\n",
    "y_pred = models.predict(X_test)\n",
    "\n",
    "# Affichage MAE, R2 et analyse des residus avec Shapiro Wilk\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2_scores = r2_score(y_test, y_pred)\n",
    "residus = y_test - y_pred\n",
    "statistic, p_value = shapiro(residus)\n",
    "\n",
    "print(f'Statistique W={statistic}, p-value={p_value}')\n",
    "if p_value > 0.05:\n",
    "    print(\"L'échantillon des résidus semble provenir d'une distribution normale\")\n",
    "else:\n",
    "    print(\"L'échantillon des résidus ne semble pas provenir d'une distribution normale\")\n",
    "print('-------------------------------')\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {r2_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique des résidus vs. valeurs prédites\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residus, color='#179269')\n",
    "plt.axhline(y=0, color='#F14512', linestyle='--', linewidth = 2)\n",
    "plt.xlabel('Valeurs Prédites')\n",
    "plt.ylabel('Résidus')\n",
    "plt.title('Résidus vs. Valeurs Prédites')\n",
    "plt.show()\n",
    "\n",
    "# Graphique de la distribution des résidus\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.histplot(residus, kde=True, color='#179269')\n",
    "ax.lines[0].set_color('#F14512')\n",
    "plt.xlabel('Résidus')\n",
    "plt.title('Distribution des Résidus')\n",
    "plt.show()\n",
    "\n",
    "# Graphique de l'importance des variables explicatives\n",
    "feature_importance = models.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center', color='#179269')\n",
    "plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])\n",
    "plt.title('Feature Importance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
